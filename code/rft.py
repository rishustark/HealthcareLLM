import torch
from transformers import TrainingArguments, Trainer, AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training
import json
from pathlib import Path
import logging
import os

# Use adapted SFT tokenization logic and BaseLLM/Data classes
from .base_llm import BaseLLM
from .data import Dataset, benchmark, DATA_DIR
from .sft import tokenize_sft, load_sft_model # Reuse SFT tokenizer and model loading logic

# Setup basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
PROJECT_DIR = Path(__file__).parent

# --- RFT Data Handling --- #

def format_example_rft(question: str, reasoning_answer: str) -> dict[str, str]:
    """
    Formats the RFT data triplet into the question/answer dict needed by tokenize_sft.
    The 'answer' here is the full reasoning string produced by CoT.

    Args:
        question: The original question.
        reasoning_answer: The full generated response string (including reasoning)
                          that led to a correct answer.

    Returns:
        Dictionary with "question" and "answer" keys for tokenization.
    """
    # The tokenize_sft function expects "question" and "answer".
    # For RFT, the target "answer" IS the full reasoning string.
    return {"question": question, "answer": reasoning_answer}

class RFTJsonlDataset:
    """Loads and tokenizes RFT data from a JSONL file."""
    def __init__(self, tokenizer, jsonl_path: str, format_fn, max_length: int = 256):
        """
        Args:
            tokenizer: Tokenizer instance.
            jsonl_path: Path to the JSONL file generated by datagen.py.
                     Expected format per line: {"question": ..., "correct_answer": ..., "reasoning_answer": ...}
            format_fn: Function to format the raw data item (e.g., format_example_rft).
            max_length: Max sequence length for tokenization.
        """
        self.format_fn = format_fn
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.raw_data = []
        self.path = Path(jsonl_path)

        try:
            with open(self.path, 'r') as f:
                for line in f:
                    try:
                        self.raw_data.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        logging.warning(f"Skipping invalid JSON line in {self.path}: {line.strip()}")
            logging.info(f"Loaded {len(self.raw_data)} RFT examples from {self.path}")
            if not self.raw_data:
                 logging.warning(f"RFT data file {self.path} is empty.")

        except FileNotFoundError:
            logging.error(f"Error: RFT data file not found at {self.path}")
            # Consider raising error or handling gracefully
            # raise
        except Exception as e:
             logging.error(f"Error reading RFT data file {self.path}: {e}")


    def __len__(self):
        return len(self.raw_data)

    def __getitem__(self, idx):
        # Get the raw data entry (a dict)
        raw_item = self.raw_data[idx]
        question = raw_item.get("question", "")
        reasoning_answer = raw_item.get("reasoning_answer", "")

        if not question or not reasoning_answer:
             logging.warning(f"Skipping RFT item at index {idx} due to missing question or reasoning_answer.")
             # Handle potentially missing data (similar to SFTTokenizedDataset)
             if idx == 0:
                 raise ValueError("First RFT item has missing data, cannot proceed.")
             first_item = self.raw_data[0]
             question = first_item.get("question", "")
             reasoning_answer = first_item.get("reasoning_answer", "")

        # Format using the specific RFT formatter
        formatted_data = self.format_fn(question, reasoning_answer)
        # Tokenize using the reused SFT tokenizer function
        # The target/label will be the token IDs of the reasoning_answer part
        return tokenize_sft(self.tokenizer, **formatted_data, max_length=self.max_length)

# --- RFT Training --- #

def train_model(
    output_dir: str = str(PROJECT_DIR / "rft_model"),
    rft_data_path: str = str(DATA_DIR / "rft_sample.jsonl"),
    checkpoint: str = "HuggingFaceTB/SmolLM-360M-Instruct", # Base model
    # --- LoRA Config --- (RFT might benefit from slightly different LoRA config)
    use_lora: bool = True,
    lora_r: int = 16, # Rank - often kept similar or slightly larger than SFT
    lora_alpha: int = 32,
    lora_dropout: float = 0.05,
    lora_target_modules: list[str] | str = "all-linear",
    # --- Quantization --- (for QLoRA)
    use_quantization: bool = True,
    # --- Training Arguments --- (Can differ from SFT)
    learning_rate: float = 1e-4, # Potentially lower LR for RFT
    num_train_epochs: int = 5, # Potentially more epochs for RFT
    per_device_train_batch_size: int = 4, # Smaller batch size often needed for longer RFT sequences
    gradient_accumulation_steps: int = 8,
    max_length: int = 512, # Allow longer sequences for reasoning chains
    logging_steps: int = 20,
    save_strategy: str = "epoch",
    optim: str = "paged_adamw_8bit",
    gradient_checkpointing: bool = True,
    warmup_ratio: float = 0.1,
    lr_scheduler_type: str = "cosine",
    report_to: str = "tensorboard",
    run_test_after_train: bool = True,
    **kwargs,
):
    """Trains the RFT model using the dataset generated by datagen.py."""
    logging.info(f"Starting RFT training...")
    logging.info(f"RFT Data Path: {rft_data_path}")
    logging.info(f"Base Model: {checkpoint}")
    logging.info(f"Output Directory: {output_dir}")
    logging.info(f"Using LoRA: {use_lora}, Using QLoRA (4-bit): {use_quantization}")
    if use_lora:
        logging.info(f"LoRA Config: r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}, target={lora_target_modules}")
    logging.info(f"Training Params: epochs={num_train_epochs}, lr={learning_rate}, batch_size={per_device_train_batch_size}, grad_accum={gradient_accumulation_steps}, max_len={max_length}")

    # --- Load Model and Tokenizer --- #
    quantization_config = None
    model_kwargs = {}
    if use_quantization:
        logging.info("Loading base model with 4-bit quantization.")
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        model_kwargs["device_map"] = "auto"

    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            checkpoint,
            quantization_config=quantization_config,
            trust_remote_code=True,
            **model_kwargs
        )
        tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)
    except Exception as e:
        logging.error(f"Failed to load base model or tokenizer from '{checkpoint}': {e}")
        return

    # --- Configure Tokenizer --- #
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        base_model.config.pad_token_id = base_model.config.eos_token_id
    tokenizer.padding_side = "right"

    # --- Prepare Model for PEFT --- #
    if use_quantization:
        base_model = prepare_model_for_kbit_training(base_model)

    model = base_model
    if use_lora:
        logging.info("Applying LoRA adapter...")
        # Configure LoRA (can reuse logic from SFT for finding linear layers)
        if isinstance(lora_target_modules, str) and lora_target_modules == "all-linear":
             import bitsandbytes as bnb
             cls = bnb.nn.Linear4bit if use_quantization else torch.nn.Linear
             target_modules_found = set()
             for name, module in base_model.named_modules():
                 if isinstance(module, cls):
                     names = name.split('.')
                     target_modules_found.add(names[-1])
             if 'lm_head' in target_modules_found: target_modules_found.remove('lm_head')
             lora_target_modules = list(target_modules_found)
             logging.info(f"Targeting LoRA for modules: {lora_target_modules}")

        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=lora_target_modules,
            bias="none",
            task_type="CAUSAL_LM",
        )
        try:
            model = get_peft_model(model, lora_config)
            logging.info("LoRA adapter applied successfully.")
            model.print_trainable_parameters()
        except Exception as e:
            logging.error(f"Failed to apply LoRA adapter: {e}")
            return
    else:
        logging.warning("Proceeding with full fine-tuning for RFT.")

    # --- Load and Tokenize Data --- #
    try:
        tokenized_rft_dataset = RFTJsonlDataset(
            tokenizer, rft_data_path, format_example_rft, max_length=max_length
        )
        if len(tokenized_rft_dataset) == 0:
            logging.error(f"RFT dataset at '{rft_data_path}' is empty or failed to load.")
            return
    except Exception as e:
        logging.error(f"Failed to load or tokenize RFT data: {e}")
        return

    # --- Configure Training --- #
    os.makedirs(output_dir, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=output_dir,
        logging_dir=f"{output_dir}/logs",
        report_to=report_to,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        optim=optim if use_quantization else "adamw_torch",
        gradient_checkpointing=gradient_checkpointing,
        fp16=not use_quantization and torch.cuda.is_available(),
        bf16=False,
        max_grad_norm=1.0,
        warmup_ratio=warmup_ratio,
        lr_scheduler_type=lr_scheduler_type,
        save_strategy=save_strategy,
        logging_steps=logging_steps,
        remove_unused_columns=False,
        **kwargs
    )

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_rft_dataset,
        tokenizer=tokenizer,
    )

    # --- Train --- #
    logging.info("Starting RFT training process...")
    try:
        trainer.train()
        logging.info("RFT Training finished successfully.")
    except Exception as e:
        logging.error(f"Error during RFT training: {e}", exc_info=True)
        return

    # --- Save Final Model --- #
    logging.info(f"Saving final RFT model adapter (or full model) to {output_dir}")
    try:
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
        logging.info("RFT Model and tokenizer saved.")
    except Exception as e:
        logging.error(f"Error saving final RFT model/tokenizer: {e}")

    if save_strategy == "steps" or save_strategy == "epoch":
        print("\nREMINDER: Manually delete intermediate 'checkpoint-XXX' folders")
        print(f"inside '{output_dir}' before distribution if desired.\n")

    # --- Optional: Run Test --- #
    if run_test_after_train:
        logging.info("Running evaluation on validation set after RFT training...")
        # Use the *same* test function from SFT, which loads the adapter correctly
        test_model(ckpt_path=output_dir, base_checkpoint=checkpoint, use_quantization_base=use_quantization)


def test_model(ckpt_path: str, base_checkpoint: str, valid_split: str = "valid_sample", use_quantization_base: bool = True):
    """Tests the trained RFT model by loading the PEFT adapter and running benchmark."""
    # This reuses the SFT test logic, which correctly handles loading PEFT models
    logging.info(f"\n--- Starting RFT Model Test --- Ckpt: {ckpt_path}")
    # Delegate to the SFT test function
    # It will load the adapter from ckpt_path onto the base_checkpoint
    # Note: The benchmark uses `parse_answer` which looks for <answer> tag.
    # RFT model outputs full reasoning. Benchmark accuracy might be low if reasoning doesn't end perfectly.
    # Consider adapting benchmark or parse_answer for RFT evaluation if needed.
    from .sft import test_model as sft_test_model
    try:
        sft_test_model(ckpt_path, base_checkpoint, valid_split, use_quantization_base)
    except Exception as e:
         logging.error(f"Error during RFT test delegation: {e}")
    logging.info("--- RFT Model Test Complete ---")

if __name__ == "__main__":
    import fire
    # Example: python -m BioHealthLLM.code.rft train --num_train_epochs 2
    # Example: python -m BioHealthLLM.code.rft test --ckpt_path="BioHealthLLM/code/rft_model" --base_checkpoint="HuggingFaceTB/SmolLM-360M-Instruct"
    fire.Fire({"train": train_model, "test": test_model}) 